{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Fh3mQh3ZFCI"
      },
      "outputs": [],
      "source": [
        "#############################################################\n",
        "# Training and Evaluation\n",
        "#############################################################\n",
        "\n",
        "class MAMLTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        maml_model,\n",
        "        train_tasks,\n",
        "        val_tasks,\n",
        "        test_tasks=None,\n",
        "        meta_epochs=10000,\n",
        "        meta_batch_size=16,\n",
        "        eval_interval=100,\n",
        "        early_stopping_patience=10,\n",
        "        log_dir='logs'\n",
        "    ):\n",
        "        self.maml_model = maml_model\n",
        "        self.train_tasks = train_tasks\n",
        "        self.val_tasks = val_tasks\n",
        "        self.test_tasks = test_tasks\n",
        "        self.meta_epochs = meta_epochs\n",
        "        self.meta_batch_size = meta_batch_size\n",
        "        self.eval_interval = eval_interval\n",
        "        self.early_stopping_patience = early_stopping_patience\n",
        "        self.log_dir = log_dir\n",
        "\n",
        "        # Create log directory if it doesn't exist\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "        # Initialize training history\n",
        "        self.history = {\n",
        "            'train_loss': [],\n",
        "            'val_accuracy': [],\n",
        "            'val_loss': []\n",
        "        }\n",
        "\n",
        "        # Initialize early stopping variables\n",
        "        self.best_val_accuracy = 0\n",
        "        self.patience_counter = 0\n",
        "        self.best_weights = None\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train the MAML model\"\"\"\n",
        "        print(\"Starting meta-training...\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(self.meta_epochs):\n",
        "            # Sample batch of tasks\n",
        "            batch_indices = np.random.choice(\n",
        "                len(self.train_tasks),\n",
        "                min(self.meta_batch_size, len(self.train_tasks)),\n",
        "                replace=False\n",
        "            )\n",
        "            batch_of_tasks = [self.train_tasks[i] for i in batch_indices]\n",
        "\n",
        "            # Train on batch of tasks\n",
        "            loss = self.maml_model.train_on_batch(batch_of_tasks)\n",
        "            self.history['train_loss'].append(loss)\n",
        "\n",
        "            # Evaluate periodically\n",
        "            if (epoch + 1) % self.eval_interval == 0:\n",
        "                val_accuracy, val_loss = self.maml_model.evaluate(self.val_tasks)\n",
        "                self.history['val_accuracy'].append(val_accuracy)\n",
        "                self.history['val_loss'].append(val_loss)\n",
        "\n",
        "                elapsed_time = time.time() - start_time\n",
        "                print(f\"Epoch {epoch+1}/{self.meta_epochs} - \"\n",
        "                      f\"Loss: {loss:.4f} - \"\n",
        "                      f\"Val Accuracy: {val_accuracy:.4f} - \"\n",
        "                      f\"Val Loss: {val_loss:.4f} - \"\n",
        "                      f\"Time: {elapsed_time:.2f}s\")\n",
        "\n",
        "                # Check for early stopping\n",
        "                if val_accuracy > self.best_val_accuracy:\n",
        "                    self.best_val_accuracy = val_accuracy\n",
        "                    self.patience_counter = 0\n",
        "                    self.best_weights = self.maml_model.model.get_weights()\n",
        "                    # Save best model\n",
        "                    self.maml_model.save_meta_model(os.path.join(self.log_dir, 'best_model.h5'))\n",
        "                else:\n",
        "                    self.patience_counter += 1\n",
        "\n",
        "                if self.patience_counter >= self.early_stopping_patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        # Restore best weights\n",
        "        if self.best_weights is not None:\n",
        "            self.maml_model.model.set_weights(self.best_weights)\n",
        "            self.maml_model.meta_weights = self.best_weights\n",
        "\n",
        "        print(f\"Meta-training completed in {time.time() - start_time:.2f}s\")\n",
        "\n",
        "        # Final evaluation on test set if available\n",
        "        if self.test_tasks is not None:\n",
        "            test_accuracy, test_loss = self.maml_model.evaluate(self.test_tasks)\n",
        "            print(f\"Test Accuracy: {test_accuracy:.4f} - Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "        return self.history\n",
        "\n",
        "    def visualize_training(self):\n",
        "        \"\"\"Visualize the training history\"\"\"\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Plot training loss\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.history['train_loss'], label='Training Loss')\n",
        "        plt.plot(np.arange(0, len(self.history['train_loss']), self.eval_interval)[:-1],\n",
        "                 self.history['val_loss'], 'r-', label='Validation Loss')\n",
        "        plt.title('Meta-Learning Loss')\n",
        "        plt.xlabel('Meta-Iterations')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "        # Plot validation accuracy\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(np.arange(0, len(self.history['train_loss']), self.eval_interval)[:-1],\n",
        "                 self.history['val_accuracy'], 'g-', label='Validation Accuracy')\n",
        "        plt.title('Few-Shot Classification Accuracy')\n",
        "        plt.xlabel('Meta-Iterations')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.log_dir, 'training_history.png'), dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_adaptation_curve(self, task, updates_range=[0, 1, 2, 3, 5, 10]):\n",
        "        \"\"\"Plot the adaptation curve for a specific task\"\"\"\n",
        "        support_X = task['support_X']\n",
        "        support_y = task['support_y']\n",
        "        query_X = task['query_X']\n",
        "        query_y = task['query_y']\n",
        "\n",
        "        accuracies = []\n",
        "\n",
        "        for updates in updates_range:\n",
        "            # Adapt model to task with different number of gradient updates\n",
        "            adapted_model = self.maml_model.adapt_to_task(support_X, support_y, num_inner_updates=updates)\n",
        "\n",
        "            # Evaluate on query set\n",
        "            query_logits = adapted_model(query_X, training=False)\n",
        "            pred_y = tf.argmax(query_logits, axis=1).numpy()\n",
        "            accuracy = np.mean(pred_y == query_y)\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(updates_range, accuracies, 'o-', linewidth=2)\n",
        "        plt.title('Adaptation Performance vs Gradient Steps')\n",
        "        plt.xlabel('Number of Gradient Updates')\n",
        "        plt.ylabel('Query Set Accuracy')\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.savefig(os.path.join(self.log_dir, 'adaptation_curve.png'), dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "        return accuracies\n"
      ]
    }
  ]
}