{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuQFZIw4XzNm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model-Agnostic Meta-Learning (MAML) with Transformer Architecture for Network Intrusion Detection\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Configure GPU if available\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "#############################################################\n",
        "# Data Loading and Preprocessing\n",
        "#############################################################\n",
        "\n",
        "class NetworkDataProcessor:\n",
        "    def __init__(self, data_path=None):\n",
        "        self.data_path = data_path\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.feature_scaler = StandardScaler()\n",
        "        self.max_seq_length = 30  # For sequential flows\n",
        "\n",
        "    def load_data(self, dataset_name=\"UNSW-NB15\"):\n",
        "        \"\"\"\n",
        "        Load network intrusion detection dataset\n",
        "        Supports: UNSW-NB15, NSL-KDD, CICIDS2017\n",
        "        \"\"\"\n",
        "        if self.data_path is None:\n",
        "            # For demo purposes, we'll use a sample from UNSW-NB15\n",
        "            print(f\"No data path provided. Using synthetic data for demonstration.\")\n",
        "            return self._generate_synthetic_data()\n",
        "\n",
        "        if dataset_name == \"UNSW-NB15\":\n",
        "            df = pd.read_csv(self.data_path)\n",
        "            # Map 'attack_cat' to labels, 'normal' remains as is\n",
        "            df['binary_label'] = df['label'].apply(lambda x: 0 if x == 0 else 1)\n",
        "            return df\n",
        "        elif dataset_name == \"NSL-KDD\":\n",
        "            df = pd.read_csv(self.data_path)\n",
        "            # Map attack types to binary labels\n",
        "            df['binary_label'] = df['label'].apply(lambda x: 0 if x == 'normal' else 1)\n",
        "            return df\n",
        "        elif dataset_name == \"CICIDS2017\":\n",
        "            df = pd.read_csv(self.data_path)\n",
        "            # Map 'Label' to binary (0 for benign, 1 for attacks)\n",
        "            df['binary_label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)\n",
        "            return df\n",
        "        else:\n",
        "            raise ValueError(f\"Dataset {dataset_name} not supported\")\n",
        "\n",
        "    def _generate_synthetic_data(self, n_samples=10000):\n",
        "        \"\"\"Generate synthetic network flow data for demonstration\"\"\"\n",
        "        # Create synthetic features resembling network flow data\n",
        "        data = {\n",
        "            'duration': np.random.exponential(scale=30, size=n_samples),\n",
        "            'protocol_type': np.random.choice(['tcp', 'udp', 'icmp'], size=n_samples),\n",
        "            'service': np.random.choice(['http', 'ftp', 'smtp', 'ssh', 'dns'], size=n_samples),\n",
        "            'src_bytes': np.random.exponential(scale=1000, size=n_samples),\n",
        "            'dst_bytes': np.random.exponential(scale=800, size=n_samples),\n",
        "            'flag': np.random.choice(['SF', 'REJ', 'S0', 'RSTO'], size=n_samples),\n",
        "            'land': np.random.choice([0, 1], size=n_samples, p=[0.99, 0.01]),\n",
        "            'wrong_fragment': np.random.choice([0, 1, 2, 3], size=n_samples, p=[0.95, 0.02, 0.02, 0.01]),\n",
        "            'urgent': np.random.choice([0, 1, 2], size=n_samples, p=[0.98, 0.01, 0.01]),\n",
        "            'hot': np.random.poisson(lam=0.1, size=n_samples),\n",
        "            'num_failed_logins': np.random.poisson(lam=0.05, size=n_samples),\n",
        "            'logged_in': np.random.choice([0, 1], size=n_samples, p=[0.4, 0.6]),\n",
        "            'num_compromised': np.random.poisson(lam=0.01, size=n_samples),\n",
        "            'root_shell': np.random.choice([0, 1], size=n_samples, p=[0.99, 0.01]),\n",
        "            'su_attempted': np.random.choice([0, 1], size=n_samples, p=[0.99, 0.01]),\n",
        "            'num_root': np.random.poisson(lam=0.01, size=n_samples),\n",
        "            'num_file_creations': np.random.poisson(lam=0.1, size=n_samples),\n",
        "            'num_shells': np.random.poisson(lam=0.01, size=n_samples),\n",
        "            'num_access_files': np.random.poisson(lam=0.05, size=n_samples),\n",
        "            'is_host_login': np.random.choice([0, 1], size=n_samples, p=[0.99, 0.01]),\n",
        "            'is_guest_login': np.random.choice([0, 1], size=n_samples, p=[0.95, 0.05]),\n",
        "        }\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Generate labels: 5 attack types + normal\n",
        "        attack_types = ['normal', 'dos', 'probe', 'r2l', 'u2r', 'backdoor']\n",
        "\n",
        "        # Create imbalanced dataset (realistic for network security)\n",
        "        # 80% normal, 20% attacks with different distributions\n",
        "        attack_probs = [0.8, 0.1, 0.04, 0.03, 0.02, 0.01]\n",
        "        df['attack_cat'] = np.random.choice(attack_types, size=n_samples, p=attack_probs)\n",
        "\n",
        "        # Add binary label (0 for normal, 1 for attack)\n",
        "        df['binary_label'] = df['attack_cat'].apply(lambda x: 0 if x == 'normal' else 1)\n",
        "\n",
        "        # Add a few more network-specific features\n",
        "        df['pkt_count'] = np.random.poisson(lam=15, size=n_samples)\n",
        "        df['byte_count'] = df['pkt_count'] * np.random.lognormal(4, 1, size=n_samples)\n",
        "        df['tcp_flags'] = np.random.choice(['000', '001', '010', '011', '100'], size=n_samples)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        \"\"\"Preprocess the data for network intrusion detection\"\"\"\n",
        "        # Make a copy to avoid modifying the original\n",
        "        processed_df = df.copy()\n",
        "\n",
        "        # Handle categorical features\n",
        "        categorical_columns = processed_df.select_dtypes(include=['object']).columns\n",
        "        for col in categorical_columns:\n",
        "            if col != 'attack_cat':  # Don't encode the target yet\n",
        "                processed_df[col] = self.label_encoder.fit_transform(processed_df[col])\n",
        "\n",
        "        # Convert attack categories to numeric labels\n",
        "        if 'attack_cat' in processed_df.columns:\n",
        "            processed_df['attack_cat_encoded'] = self.label_encoder.fit_transform(processed_df['attack_cat'])\n",
        "            self.attack_mapping = dict(zip(self.label_encoder.classes_, self.label_encoder.transform(self.label_encoder.classes_)))\n",
        "            print(\"Attack mapping:\", self.attack_mapping)\n",
        "\n",
        "        # Extract features and labels\n",
        "        if 'attack_cat' in processed_df.columns:\n",
        "            X = processed_df.drop(['attack_cat', 'attack_cat_encoded', 'binary_label'], axis=1, errors='ignore')\n",
        "            y_multiclass = processed_df['attack_cat_encoded'] if 'attack_cat_encoded' in processed_df.columns else None\n",
        "            y_binary = processed_df['binary_label']\n",
        "        else:\n",
        "            X = processed_df.drop(['binary_label'], axis=1, errors='ignore')\n",
        "            y_multiclass = None\n",
        "            y_binary = processed_df['binary_label']\n",
        "\n",
        "        # Handle missing values\n",
        "        X = X.fillna(0)\n",
        "\n",
        "        # Scale numerical features\n",
        "        X_scaled = self.feature_scaler.fit_transform(X)\n",
        "\n",
        "        return X_scaled, y_binary, y_multiclass\n",
        "\n",
        "    def create_tasks(self, X, y_multiclass, num_tasks=100, k_shot=5, query_size=15):\n",
        "        \"\"\"\n",
        "        Create tasks for meta-learning\n",
        "        Each task contains:\n",
        "        - support set: k examples of N classes (k-shot, N-way)\n",
        "        - query set: query_size examples of the same N classes\n",
        "        \"\"\"\n",
        "        if y_multiclass is None:\n",
        "            raise ValueError(\"Multiclass labels are required for creating few-shot tasks\")\n",
        "\n",
        "        # Get unique classes\n",
        "        classes = np.unique(y_multiclass)\n",
        "        n_classes = len(classes)\n",
        "        n_way = min(5, n_classes)  # Default to 5-way classification if possible\n",
        "\n",
        "        tasks = []\n",
        "        for _ in range(num_tasks):\n",
        "            # Randomly select N classes for this task\n",
        "            task_classes = np.random.choice(classes, n_way, replace=False)\n",
        "\n",
        "            support_X, support_y = [], []\n",
        "            query_X, query_y = [], []\n",
        "\n",
        "            for class_idx, cls in enumerate(task_classes):\n",
        "                # Find examples of this class\n",
        "                cls_indices = np.where(y_multiclass == cls)[0]\n",
        "\n",
        "                # Ensure we have enough examples\n",
        "                if len(cls_indices) < k_shot + query_size:\n",
        "                    # If not enough examples, sample with replacement\n",
        "                    selected_indices = np.random.choice(cls_indices, k_shot + query_size, replace=True)\n",
        "                else:\n",
        "                    # Otherwise, sample without replacement\n",
        "                    selected_indices = np.random.choice(cls_indices, k_shot + query_size, replace=False)\n",
        "\n",
        "                # Split into support and query\n",
        "                support_indices = selected_indices[:k_shot]\n",
        "                query_indices = selected_indices[k_shot:k_shot + query_size]\n",
        "\n",
        "                # Add to support set\n",
        "                support_X.append(X[support_indices])\n",
        "                support_y.append(np.full(k_shot, class_idx))  # Use task-specific class indices\n",
        "\n",
        "                # Add to query set\n",
        "                query_X.append(X[query_indices])\n",
        "                query_y.append(np.full(query_size, class_idx))  # Use task-specific class indices\n",
        "\n",
        "            # Combine and shuffle support set\n",
        "            support_X = np.vstack(support_X)\n",
        "            support_y = np.concatenate(support_y)\n",
        "            support_indices = np.arange(len(support_y))\n",
        "            np.random.shuffle(support_indices)\n",
        "            support_X = support_X[support_indices]\n",
        "            support_y = support_y[support_indices]\n",
        "\n",
        "            # Combine and shuffle query set\n",
        "            query_X = np.vstack(query_X)\n",
        "            query_y = np.concatenate(query_y)\n",
        "            query_indices = np.arange(len(query_y))\n",
        "            np.random.shuffle(query_indices)\n",
        "            query_X = query_X[query_indices]\n",
        "            query_y = query_y[query_indices]\n",
        "\n",
        "            # Create a task dictionary\n",
        "            task = {\n",
        "                'support_X': support_X,\n",
        "                'support_y': support_y,\n",
        "                'query_X': query_X,\n",
        "                'query_y': query_y,\n",
        "                'n_way': n_way,\n",
        "                'k_shot': k_shot,\n",
        "                'classes': task_classes\n",
        "            }\n",
        "\n",
        "            tasks.append(task)\n",
        "\n",
        "        return tasks\n",
        "\n",
        "\n",
        "#############################################################\n",
        "# Transformer Architecture\n",
        "#############################################################\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"att\": self.att,\n",
        "            \"ffn\": self.ffn,\n",
        "            \"layernorm1\": self.layernorm1,\n",
        "            \"layernorm2\": self.layernorm2,\n",
        "            \"dropout1\": self.dropout1,\n",
        "            \"dropout2\": self.dropout2,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "            d_model=d_model\n",
        "        )\n",
        "\n",
        "        # Apply sin to even indices in the array; 2i\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "\n",
        "        # Apply cos to odd indices in the array; 2i+1\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"pos_encoding\": self.pos_encoding,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "#############################################################\n",
        "# MAML Implementation\n",
        "#############################################################\n",
        "\n",
        "class MAMLTransformer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_shape,\n",
        "        n_way=5,\n",
        "        k_shot=5,\n",
        "        inner_lr=0.01,\n",
        "        meta_lr=0.001,\n",
        "        meta_batch_size=32,\n",
        "        num_inner_updates=5,\n",
        "        embed_dim=256,\n",
        "        num_heads=8,\n",
        "        ff_dim=512,\n",
        "        num_transformer_blocks=4,\n",
        "        mlp_units=[128, 64],\n",
        "        dropout=0.1,\n",
        "        name=\"maml_transformer\"\n",
        "    ):\n",
        "        self.input_shape = input_shape\n",
        "        self.n_way = n_way\n",
        "        self.k_shot = k_shot\n",
        "        self.inner_lr = inner_lr\n",
        "        self.meta_lr = meta_lr\n",
        "        self.meta_batch_size = meta_batch_size\n",
        "        self.num_inner_updates = num_inner_updates\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.num_transformer_blocks = num_transformer_blocks\n",
        "        self.mlp_units = mlp_units\n",
        "        self.dropout = dropout\n",
        "        self.name = name\n",
        "\n",
        "        # Build the model\n",
        "        self.model = self._build_model()\n",
        "\n",
        "        # Initialize optimizer\n",
        "        self.meta_optimizer = keras.optimizers.Adam(learning_rate=self.meta_lr)\n",
        "\n",
        "        # Store the original weights\n",
        "        self.meta_weights = self.model.get_weights()\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"Build the transformer-based model for few-shot learning\"\"\"\n",
        "        inputs = layers.Input(shape=self.input_shape)\n",
        "\n",
        "        # Embedding layer to transform input features to embedding space\n",
        "        x = layers.Dense(self.embed_dim)(inputs)\n",
        "\n",
        "        # Reshape for transformer if input is not sequential\n",
        "        # For sequential data, we would reshape to (batch_size, seq_length, embed_dim)\n",
        "        # For tabular data, we treat each feature as a \"token\"\n",
        "        x = tf.expand_dims(x, axis=1)  # Add sequence dimension of length 1\n",
        "\n",
        "        # Apply positional encoding\n",
        "        x = PositionalEncoding(position=50, d_model=self.embed_dim)(x)\n",
        "\n",
        "        # Transformer blocks\n",
        "        for _ in range(self.num_transformer_blocks):\n",
        "            x = TransformerBlock(\n",
        "                embed_dim=self.embed_dim,\n",
        "                num_heads=self.num_heads,\n",
        "                ff_dim=self.ff_dim,\n",
        "                rate=self.dropout\n",
        "            )(x)\n",
        "\n",
        "        # Global average pooling\n",
        "        x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "        # MLP for classification\n",
        "        for dim in self.mlp_units:\n",
        "            x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "            x = layers.Dropout(self.dropout)(x)\n",
        "\n",
        "        # Output layer\n",
        "        outputs = layers.Dense(self.n_way, activation=\"softmax\")(x)\n",
        "\n",
        "        return keras.Model(inputs, outputs, name=self.name)\n",
        "\n",
        "    def train_on_batch(self, batch_of_tasks):\n",
        "        \"\"\"\n",
        "        Train on a batch of tasks using MAML\n",
        "\n",
        "        Args:\n",
        "            batch_of_tasks: list of tasks, each containing support and query sets\n",
        "        \"\"\"\n",
        "        meta_batch_size = min(len(batch_of_tasks), self.meta_batch_size)\n",
        "\n",
        "        with tf.GradientTape() as meta_tape:\n",
        "            total_meta_loss = tf.constant(0.0, dtype=tf.float32)\n",
        "\n",
        "            for i in range(meta_batch_size):\n",
        "                task = batch_of_tasks[i]\n",
        "                support_X = task['support_X']\n",
        "                support_y = task['support_y']\n",
        "                query_X = task['query_X']\n",
        "                query_y = task['query_y']\n",
        "\n",
        "                # Convert to one-hot\n",
        "                support_y_one_hot = tf.one_hot(support_y, depth=self.n_way)\n",
        "                query_y_one_hot = tf.one_hot(query_y, depth=self.n_way)\n",
        "\n",
        "                # Store the original weights\n",
        "                original_weights = self.model.get_weights()\n",
        "\n",
        "                # Inner loop - adapt to the task\n",
        "                for _ in range(self.num_inner_updates):\n",
        "                    with tf.GradientTape() as inner_tape:\n",
        "                        support_logits = self.model(support_X, training=True)\n",
        "                        support_loss = keras.losses.categorical_crossentropy(\n",
        "                            support_y_one_hot, support_logits\n",
        "                        )\n",
        "                        support_loss = tf.reduce_mean(support_loss)\n",
        "\n",
        "                    # Compute gradients and update model weights\n",
        "                    gradients = inner_tape.gradient(support_loss, self.model.trainable_variables)\n",
        "\n",
        "                    # Manual weight update for inner loop\n",
        "                    updated_weights = []\n",
        "                    for j, (weight, grad) in enumerate(zip(self.model.get_weights(), gradients)):\n",
        "                        if grad is not None:\n",
        "                            updated_weights.append(weight - self.inner_lr * grad)\n",
        "                        else:\n",
        "                            updated_weights.append(weight)\n",
        "\n",
        "                    # Set the updated weights\n",
        "                    self.model.set_weights(updated_weights)\n",
        "\n",
        "                # Evaluate on query set with the adapted model\n",
        "                query_logits = self.model(query_X, training=True)\n",
        "                query_loss = keras.losses.categorical_crossentropy(\n",
        "                    query_y_one_hot, query_logits\n",
        "                )\n",
        "                query_loss = tf.reduce_mean(query_loss)\n",
        "\n",
        "                # Add to meta loss\n",
        "                total_meta_loss += query_loss\n",
        "\n",
        "                # Restore original weights\n",
        "                self.model.set_weights(original_weights)\n",
        "\n",
        "            # Average meta loss\n",
        "            total_meta_loss /= meta_batch_size\n",
        "\n",
        "        # Compute gradients of meta loss with respect to meta weights\n",
        "        meta_gradients = meta_tape.gradient(total_meta_loss, self.model.trainable_variables)\n",
        "\n",
        "        # Apply meta gradients\n",
        "        self.meta_optimizer.apply_gradients(zip(meta_gradients, self.model.trainable_variables))\n",
        "\n",
        "        # Update stored meta weights\n",
        "        self.meta_weights = self.model.get_weights()\n",
        "\n",
        "        return total_meta_loss.numpy()\n",
        "\n",
        "    def evaluate(self, tasks, num_inner_updates=None):\n",
        "        \"\"\"\n",
        "        Evaluate the meta-model on a list of tasks\n",
        "\n",
        "        Args:\n",
        "            tasks: list of tasks to evaluate on\n",
        "            num_inner_updates: number of inner updates to perform (if None, use self.num_inner_updates)\n",
        "\n",
        "        Returns:\n",
        "            mean accuracy across tasks\n",
        "        \"\"\"\n",
        "        if num_inner_updates is None:\n",
        "            num_inner_updates = self.num_inner_updates\n",
        "\n",
        "        accuracies = []\n",
        "        losses = []\n",
        "\n",
        "        # Restore meta weights\n",
        "        self.model.set_weights(self.meta_weights)\n",
        "\n",
        "        for task in tasks:\n",
        "            support_X = task['support_X']\n",
        "            support_y = task['support_y']\n",
        "            query_X = task['query_X']\n",
        "            query_y = task['query_y']\n",
        "\n",
        "            # Convert to one-hot\n",
        "            support_y_one_hot = tf.one_hot(support_y, depth=self.n_way)\n",
        "\n",
        "            # Store the original weights\n",
        "            original_weights = self.model.get_weights()\n",
        "\n",
        "            # Inner loop adaptation\n",
        "            for _ in range(num_inner_updates):\n",
        "                with tf.GradientTape() as tape:\n",
        "                    support_logits = self.model(support_X, training=True)\n",
        "                    support_loss = keras.losses.categorical_crossentropy(\n",
        "                        support_y_one_hot, support_logits\n",
        "                    )\n",
        "                    support_loss = tf.reduce_mean(support_loss)\n",
        "\n",
        "                # Compute gradients and update model weights\n",
        "                gradients = tape.gradient(support_loss, self.model.trainable_variables)\n",
        "\n",
        "                # Manual weight update\n",
        "                updated_weights = []\n",
        "                for j, (weight, grad) in enumerate(zip(self.model.get_weights(), gradients)):\n",
        "                    if grad is not None:\n",
        "                        updated_weights.append(weight - self.inner_lr * grad)\n",
        "                    else:\n",
        "                        updated_weights.append(weight)\n",
        "\n",
        "                # Set the updated weights\n",
        "                self.model.set_weights(updated_weights)\n",
        "\n",
        "            # Evaluate on query set\n",
        "            query_logits = self.model(query_X, training=False)\n",
        "            pred_y = tf.argmax(query_logits, axis=1).numpy()\n",
        "            accuracy = np.mean(pred_y == query_y)\n",
        "\n",
        "            # Calculate loss\n",
        "            query_y_one_hot = tf.one_hot(query_y, depth=self.n_way)\n",
        "            loss = keras.losses.categorical_crossentropy(query_y_one_hot, query_logits)\n",
        "            loss = tf.reduce_mean(loss).numpy()\n",
        "\n",
        "            accuracies.append(accuracy)\n",
        "            losses.append(loss)\n",
        "\n",
        "            # Restore original weights\n",
        "            self.model.set_weights(original_weights)\n",
        "\n",
        "        # Return mean accuracy and loss\n",
        "        return np.mean(accuracies), np.mean(losses)\n",
        "\n",
        "    def adapt_to_task(self, support_X, support_y, num_inner_updates=None):\n",
        "        \"\"\"\n",
        "        Adapt the model to a new task using the support set\n",
        "\n",
        "        Args:\n",
        "            support_X: support set features\n",
        "            support_y: support set labels\n",
        "            num_inner_updates: number of inner updates to perform (if None, use self.num_inner_updates)\n",
        "\n",
        "        Returns:\n",
        "            Adapted model\n",
        "        \"\"\"\n",
        "        if num_inner_updates is None:\n",
        "            num_inner_updates = self.num_inner_updates\n",
        "\n",
        "        # Convert class indices to one-hot\n",
        "        support_y_one_hot = tf.one_hot(support_y, depth=self.n_way)\n",
        "\n",
        "        # Reset to the meta weights\n",
        "        self.model.set_weights(self.meta_weights)\n",
        "\n",
        "        # Inner loop adaptation\n",
        "        for _ in range(num_inner_updates):\n",
        "            with tf.GradientTape() as tape:\n",
        "                support_logits = self.model(support_X, training=True)\n",
        "                support_loss = keras.losses.categorical_crossentropy(\n",
        "                    support_y_one_hot, support_logits\n",
        "                )\n",
        "                support_loss = tf.reduce_mean(support_loss)\n",
        "\n",
        "            # Compute gradients\n",
        "            gradients = tape.gradient(support_loss, self.model.trainable_variables)\n",
        "\n",
        "            # Manual weight update\n",
        "            updated_weights = []\n",
        "            for j, (weight, grad) in enumerate(zip(self.model.get_weights(), gradients)):\n",
        "                if grad is not None:\n",
        "                    updated_weights.append(weight - self.inner_lr * grad)\n",
        "                else:\n",
        "                    updated_weights.append(weight)\n",
        "\n",
        "            # Set the updated weights\n",
        "            self.model.set_weights(updated_weights)\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def save_meta_model(self, filepath):\n",
        "        \"\"\"Save the meta-model weights\"\"\"\n",
        "        # Set to meta weights before saving\n",
        "        self.model.set_weights(self.meta_weights)\n",
        "        self.model.save_weights(filepath)\n",
        "\n",
        "    def load_meta_model(self, filepath):\n",
        "        \"\"\"Load the meta-model weights\"\"\"\n",
        "        self.model.load_weights(filepath)\n",
        "        self.meta_weights = self.model.get_weights()\n",
        "\n",
        "\n",
        "#############################################################\n",
        "# Training and Evaluation\n",
        "#############################################################\n",
        "\n",
        "class MAMLTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        maml_model,\n",
        "        train_tasks,\n",
        "        val_tasks,\n",
        "        test_tasks=None,\n",
        "        meta_epochs=10000,\n",
        "        meta_batch_size=16,\n",
        "        eval_interval=100,\n",
        "        early_stopping_patience=10,\n",
        "        log_dir='logs'\n",
        "    ):\n",
        "        self.maml_model = maml_model\n",
        "        self.train_tasks = train_tasks\n",
        "        self.val_tasks = val_tasks\n",
        "        self.test_tasks = test_tasks\n",
        "        self.meta_epochs = meta_epochs\n",
        "        self.meta_batch_size = meta_batch_size\n",
        "        self.eval_interval = eval_interval\n",
        "        self.early_stopping_patience = early_stopping_patience\n",
        "        self.log_dir = log_dir\n",
        "\n",
        "        # Create log directory if it doesn't exist\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "        # Initialize training history\n",
        "        self.history = {\n",
        "            'train_loss': [],\n",
        "            'val_accuracy': [],\n",
        "            'val_loss': []\n",
        "        }\n",
        "\n",
        "        # Initialize early stopping variables\n",
        "        self.best_val_accuracy = 0\n",
        "        self.patience_counter = 0\n",
        "        self.best_weights = None\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Train the MAML model\"\"\"\n",
        "        print(\"Starting meta-training...\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(self.meta_epochs):\n",
        "            # Sample batch of tasks\n",
        "            batch_indices = np.random.choice(\n",
        "                len(self.train_tasks),\n",
        "                min(self.meta_batch_size, len(self.train_tasks)),\n",
        "                replace=False\n",
        "            )\n",
        "            batch_of_tasks = [self.train_tasks[i] for i in batch_indices]\n",
        "\n",
        "            # Train on batch of tasks\n",
        "            loss = self.maml_model.train_on_batch(batch_of_tasks)\n",
        "            self.history['train_loss'].append(loss)\n",
        "\n",
        "            # Evaluate periodically\n",
        "            if (epoch + 1) % self.eval_interval == 0:\n",
        "                val_accuracy, val_loss = self.maml_model.evaluate(self.val_tasks)\n",
        "                self.history['val_accuracy'].append(val_accuracy)\n",
        "                self.history['val_loss'].append(val_loss)\n",
        "\n",
        "                elapsed_time = time.time() - start_time\n",
        "                print(f\"Epoch {epoch+1}/{self.meta_epochs} - \"\n",
        "                      f\"Loss: {loss:.4f} - \"\n",
        "                      f\"Val Accuracy: {val_accuracy:.4f} - \"\n",
        "                      f\"Val Loss: {val_loss:.4f} - \"\n",
        "                      f\"Time: {elapsed_time:.2f}s\")\n",
        "\n",
        "                # Check for early stopping\n",
        "                if val_accuracy > self.best_val_accuracy:\n",
        "                    self.best_val_accuracy = val_accuracy\n",
        "                    self.patience_counter = 0\n",
        "                    self.best_weights = self.maml_model.model.get_weights()\n",
        "                    # Save best model\n",
        "                    self.maml_model.save_meta_model(os.path.join(self.log_dir, 'best_model.h5'))\n",
        "                else:\n",
        "                    self.patience_counter += 1\n",
        "\n",
        "                if self.patience_counter >= self.early_stopping_patience:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        # Restore best weights\n",
        "        if self.best_weights is not None:\n",
        "            self.maml_model.model.set_weights(self.best_weights)\n",
        "            self.maml_model.meta_weights = self.best_weights\n",
        "\n",
        "        print(f\"Meta-training completed in {time.time() - start_time:.2f}s\")\n",
        "\n",
        "        # Final evaluation on test set if available\n",
        "        if self.test_tasks is not None:\n",
        "            test_accuracy, test_loss = self.maml_model.evaluate(self.test_tasks)\n",
        "            print(f\"Test Accuracy: {test_accuracy:.4f} - Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "        return self.history\n",
        "\n",
        "    def visualize_training(self):\n",
        "        \"\"\"Visualize the training history\"\"\"\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Plot training loss\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.history['train_loss'], label='Training Loss')\n",
        "        plt.plot(np.arange(0, len(self.history['train_loss']), self.eval_interval)[:-1],\n",
        "                 self.history['val_loss'], 'r-', label='Validation Loss')\n",
        "        plt.title('Meta-Learning Loss')\n",
        "        plt.xlabel('Meta-Iterations')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "        # Plot validation accuracy\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(np.arange(0, len(self.history['train_loss']), self.eval_interval)[:-1],\n",
        "                 self.history['val_accuracy'], 'g-', label='Validation Accuracy')\n",
        "        plt.title('Few-Shot Classification Accuracy')\n",
        "        plt.xlabel('Meta-Iterations')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.log_dir, 'training_history.png'), dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_adaptation_curve(self, task, updates_range=[0, 1, 2, 3, 5, 10]):\n",
        "        \"\"\"Plot the adaptation curve for a specific task\"\"\"\n",
        "        support_X = task['support_X']\n",
        "        support_y = task['support_y']\n",
        "        query_X = task['query_X']\n",
        "        query_y = task['query_y']\n",
        "\n",
        "        accuracies = []\n",
        "\n",
        "        for updates in updates_range:\n",
        "            # Adapt model to task with different number of gradient updates\n",
        "            adapted_model = self.maml_model.adapt_to_task(support_X, support_y, num_inner_updates=updates)\n",
        "\n",
        "            # Evaluate on query set\n",
        "            query_logits = adapted_model(query_X, training=False)\n",
        "            pred_y = tf.argmax(query_logits, axis=1).numpy()\n",
        "            accuracy = np.mean(pred_y == query_y)\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(updates_range, accuracies, 'o-', linewidth=2)\n",
        "        plt.title('Adaptation Performance vs Gradient Steps')\n",
        "        plt.xlabel('Number of Gradient Updates')\n",
        "        plt.ylabel('Query Set Accuracy')\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.savefig(os.path.join(self.log_dir, 'adaptation_curve.png'), dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "        return accuracies\n",
        "\n",
        "\n",
        "#############################################################\n",
        "# Network Security Scenario Analysis\n",
        "#############################################################\n",
        "\n",
        "class NetworkSecurityAnalyzer:\n",
        "    def __init__(self, maml_model, data_processor, log_dir='logs'):\n",
        "        self.maml_model = maml_model\n",
        "        self.data_processor = data_processor\n",
        "        self.log_dir = log_dir\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    def analyze_novel_attack(self, X, y, attack_indices, normal_indices, n_shot=5):\n",
        "        \"\"\"\n",
        "        Analyze the model's ability to detect a novel attack type\n",
        "\n",
        "        Args:\n",
        "            X: Feature matrix\n",
        "            y: Binary labels\n",
        "            attack_indices: Indices of examples of the novel attack type\n",
        "            normal_indices: Indices of normal traffic\n",
        "            n_shot: Number of examples to use for adaptation\n",
        "        \"\"\"\n",
        "        # Ensure we have enough examples\n",
        "        if len(attack_indices) < n_shot * 2:\n",
        "            print(f\"Warning: Not enough attack examples. Using {len(attack_indices) // 2} shots instead.\")\n",
        "            n_shot = len(attack_indices) // 2\n",
        "\n",
        "        # Split attack indices into support and query sets\n",
        "        attack_support_indices = attack_indices[:n_shot]\n",
        "        attack_query_indices = attack_indices[n_shot:2*n_shot]\n",
        "\n",
        "        # Select normal examples for support and query sets\n",
        "        normal_support_indices = np.random.choice(normal_indices, n_shot, replace=False)\n",
        "        normal_query_indices = np.random.choice(\n",
        "            [i for i in normal_indices if i not in normal_support_indices],\n",
        "            n_shot,\n",
        "            replace=False\n",
        "        )\n",
        "\n",
        "        # Create binary classification task (normal vs attack)\n",
        "        support_indices = np.concatenate([normal_support_indices, attack_support_indices])\n",
        "        query_indices = np.concatenate([normal_query_indices, attack_query_indices])\n",
        "\n",
        "        support_X = X[support_indices]\n",
        "        support_y = np.concatenate([np.zeros(n_shot), np.ones(n_shot)])\n",
        "        query_X = X[query_indices]\n",
        "        query_y = np.concatenate([np.zeros(n_shot), np.ones(n_shot)])\n",
        "\n",
        "        # Shuffle support set\n",
        "        support_shuffle = np.arange(len(support_y))\n",
        "        np.random.shuffle(support_shuffle)\n",
        "        support_X = support_X[support_shuffle]\n",
        "        support_y = support_y[support_shuffle]\n",
        "\n",
        "        # Create task dictionary\n",
        "        task = {\n",
        "            'support_X': support_X,\n",
        "            'support_y': support_y,\n",
        "            'query_X': query_X,\n",
        "            'query_y': query_y,\n",
        "            'n_way': 2,  # Binary classification\n",
        "            'k_shot': n_shot\n",
        "        }\n",
        "\n",
        "        # Evaluate adaptation performance\n",
        "        accuracies = []\n",
        "        precisions = []\n",
        "        recalls = []\n",
        "        f1_scores = []\n",
        "        confusion_matrices = []\n",
        "\n",
        "        update_steps = [0, 1, 3, 5, 10]\n",
        "\n",
        "        for steps in update_steps:\n",
        "            # Adapt the model to the task\n",
        "            adapted_model = self.maml_model.adapt_to_task(support_X, support_y, num_inner_updates=steps)\n",
        "\n",
        "            # Make predictions on query set\n",
        "            query_logits = adapted_model(query_X, training=False)\n",
        "            pred_y = tf.argmax(query_logits, axis=1).numpy()\n",
        "\n",
        "            # Calculate metrics\n",
        "            accuracy = np.mean(pred_y == query_y)\n",
        "            cm = confusion_matrix(query_y, pred_y)\n",
        "\n",
        "            # Calculate precision, recall, and F1 for the attack class (label 1)\n",
        "            tn, fp, fn, tp = cm.ravel()\n",
        "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "            accuracies.append(accuracy)\n",
        "            precisions.append(precision)\n",
        "            recalls.append(recall)\n",
        "            f1_scores.append(f1)\n",
        "            confusion_matrices.append(cm)\n",
        "\n",
        "        # Plot adaptation results\n",
        "        self._plot_adaptation_metrics(update_steps, accuracies, precisions, recalls, f1_scores)\n",
        "\n",
        "        # Plot confusion matrix for best step\n",
        "        best_idx = np.argmax(f1_scores)\n",
        "        self._plot_confusion_matrix(confusion_matrices[best_idx], ['Normal', 'Attack'])\n",
        "\n",
        "        return {\n",
        "            'accuracies': accuracies,\n",
        "            'precisions': precisions,\n",
        "            'recalls': recalls,\n",
        "            'f1_scores': f1_scores,\n",
        "            'best_steps': update_steps[best_idx],\n",
        "            'best_f1': f1_scores[best_idx]\n",
        "        }\n",
        "\n",
        "    def _plot_adaptation_metrics(self, steps, accuracies, precisions, recalls, f1_scores):\n",
        "        \"\"\"Plot adaptation metrics vs gradient steps\"\"\"\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        plt.plot(steps, accuracies, 'o-', label='Accuracy', linewidth=2)\n",
        "        plt.plot(steps, precisions, 's-', label='Precision', linewidth=2)\n",
        "        plt.plot(steps, recalls, '^-', label='Recall', linewidth=2)\n",
        "        plt.plot(steps, f1_scores, 'D-', label='F1 Score', linewidth=2)\n",
        "\n",
        "        plt.title('Attack Detection Performance vs Adaptation Steps')\n",
        "        plt.xlabel('Number of Gradient Updates')\n",
        "        plt.ylabel('Metric Value')\n",
        "        plt.legend()\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.savefig(os.path.join(self.log_dir, 'adaptation_metrics.png'), dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    def _plot_confusion_matrix(self, cm, classes):\n",
        "        \"\"\"Plot confusion matrix\"\"\"\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.log_dir, 'confusion_matrix.png'), dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    def analyze_attack_types(self, tasks, attack_names=None):\n",
        "        \"\"\"\n",
        "        Analyze performance across different attack types\n",
        "\n",
        "        Args:\n",
        "            tasks: List of few-shot tasks for different attack types\n",
        "            attack_names: List of attack type names corresponding to tasks\n",
        "        \"\"\"\n",
        "        if attack_names is None:\n",
        "            attack_names = [f\"Attack Type {i+1}\" for i in range(len(tasks))]\n",
        "\n",
        "        n_attacks = len(tasks)\n",
        "        accuracies = []\n",
        "        f1_scores = []\n",
        "\n",
        "        for i, task in enumerate(tasks):\n",
        "            # Adapt the model to the task\n",
        "            adapted_model = self.maml_model.adapt_to_task(\n",
        "                task['support_X'], task['support_y'], num_inner_updates=5\n",
        "            )\n",
        "\n",
        "            # Make predictions on query set\n",
        "            query_X = task['query_X']\n",
        "            query_y = task['query_y']\n",
        "            query_logits = adapted_model(query_X, training=False)\n",
        "            pred_y = tf.argmax(query_logits, axis=1).numpy()\n",
        "\n",
        "            # Calculate metrics\n",
        "            accuracy = np.mean(pred_y == query_y)\n",
        "            cm = confusion_matrix(query_y, pred_y)\n",
        "\n",
        "            # For binary classification (assuming class 1 is the attack)\n",
        "            if task['n_way'] == 2:\n",
        "                tn, fp, fn, tp = cm.ravel()\n",
        "                precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "                recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "            else:\n",
        "                # For multiclass, use macro-averaged F1\n",
        "                report = classification_report(query_y, pred_y, output_dict=True)\n",
        "                f1 = report['macro avg']['f1-score']\n",
        "\n",
        "            accuracies.append(accuracy)\n",
        "            f1_scores.append(f1)\n",
        "\n",
        "        # Plot results\n",
        "        self._plot_attack_performance(attack_names, accuracies, f1_scores)\n",
        "\n",
        "        return {\n",
        "            'attack_names': attack_names,\n",
        "            'accuracies': accuracies,\n",
        "            'f1_scores': f1_scores\n",
        "        }\n",
        "\n",
        "    def _plot_attack_performance(self, attack_names, accuracies, f1_scores):\n",
        "        \"\"\"Plot performance across different attack types\"\"\"\n",
        "        plt.figure(figsize=(12, 6))\n",
        "\n",
        "        x = np.arange(len(attack_names))\n",
        "        width = 0.35\n",
        "\n",
        "        plt.bar(x - width/2, accuracies, width, label='Accuracy')\n",
        "        plt.bar(x + width/2, f1_scores, width, label='F1 Score')\n",
        "\n",
        "        plt.xlabel('Attack Type')\n",
        "        plt.ylabel('Score')\n",
        "        plt.title('Performance Across Different Attack Types')\n",
        "        plt.xticks(x, attack_names, rotation=45, ha='right')\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.savefig(os.path.join(self.log_dir, 'attack_performance.png'), dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_attention_weights(self, X, y, attack_type_idx=None):\n",
        "        \"\"\"\n",
        "        Visualize attention weights to explain model decisions\n",
        "\n",
        "        Args:\n",
        "            X: Feature matrix\n",
        "            y: Labels\n",
        "            attack_type_idx: Optional index of attack type to visualize\n",
        "        \"\"\"\n",
        "        # Get a sample\n",
        "        if attack_type_idx is not None:\n",
        "            sample_idx = np.where(y == attack_type_idx)[0][0]\n",
        "        else:\n",
        "            sample_idx = np.random.choice(len(y))\n",
        "\n",
        "        sample_X = X[sample_idx:sample_idx+1]\n",
        "\n",
        "        # Create a transformer model with accessible attention weights\n",
        "        input_shape = X.shape[1:]\n",
        "        embed_dim = self.maml_model.embed_dim\n",
        "        num_heads = self.maml_model.num_heads\n",
        "        ff_dim = self.maml_model.ff_dim\n",
        "\n",
        "        # A simplified transformer model for visualization\n",
        "        inputs = layers.Input(shape=input_shape)\n",
        "        x = layers.Dense(embed_dim)(inputs)\n",
        "        x = tf.expand_dims(x, axis=1)  # Add sequence dimension\n",
        "\n",
        "        # Add positional encoding\n",
        "        x = PositionalEncoding(position=50, d_model=embed_dim)(x)\n",
        "\n",
        "        # Use the first transformer block for visualization\n",
        "        attention_layer = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        attn_output = attention_layer(x, x)\n",
        "\n",
        "        # Create the model\n",
        "        vis_model = keras.Model(inputs=inputs, outputs=attn_output)\n",
        "\n",
        "        # Copy weights from trained model (first layer only)\n",
        "        vis_model.layers[1].set_weights(self.maml_model.model.layers[1].get_weights())\n",
        "\n",
        "        # Run the model to get attention outputs\n",
        "        attention_output = vis_model(sample_X)\n",
        "\n",
        "        # Feature names (for visualization)\n",
        "        if hasattr(self.data_processor, 'X_columns'):\n",
        "            feature_names = self.data_processor.X_columns\n",
        "        else:\n",
        "            feature_names = [f\"Feature_{i}\" for i in range(input_shape[0])]\n",
        "\n",
        "        # Plot attention heatmap\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(attention_output[0].numpy(), cmap='viridis')\n",
        "        plt.title(f\"Attention Heatmap for Sample (Class {y[sample_idx]})\")\n",
        "        plt.xlabel(\"Embedded Features\")\n",
        "        plt.ylabel(\"Sequence Position\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.log_dir, 'attention_heatmap.png'), dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "        # Return the most important features based on attention\n",
        "        feature_importance = attention_output[0].numpy().mean(axis=1).flatten()\n",
        "        top_k = 10  # Top k important features\n",
        "        top_indices = np.argsort(feature_importance)[-top_k:]\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.barh(np.array(feature_names)[top_indices], feature_importance[top_indices])\n",
        "        plt.xlabel(\"Average Attention\")\n",
        "        plt.ylabel(\"Feature\")\n",
        "        plt.title(\"Top Features by Attention Weight\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.log_dir, 'feature_importance.png'), dpi=300)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "#############################################################\n",
        "# End-to-End Pipeline Demo\n",
        "#############################################################\n",
        "\n",
        "def run_demo(data_path=None, dataset_name=\"synthetic\"):\n",
        "    \"\"\"Run a complete demo of the MAML transformer for network intrusion detection\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"MAML Transformer for Network Intrusion Detection - Demo\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Step 1: Data preparation\n",
        "    print(\"\\n1. Loading and preprocessing data...\")\n",
        "    data_processor = NetworkDataProcessor(data_path)\n",
        "\n",
        "    if dataset_name == \"synthetic\":\n",
        "        df = data_processor._generate_synthetic_data(n_samples=10000)\n",
        "    else:\n",
        "        df = data_processor.load_data(dataset_name)\n",
        "\n",
        "    X, y_binary, y_multiclass = data_processor.preprocess_data(df)\n",
        "\n",
        "    print(f\"Total samples: {len(X)}\")\n",
        "    print(f\"Features: {X.shape[1]}\")\n",
        "    if y_multiclass is not None:\n",
        "        print(f\"Number of attack classes: {len(np.unique(y_multiclass))}\")\n",
        "    print(f\"Attack samples: {np.sum(y_binary)}\")\n",
        "    print(f\"Normal samples: {len(y_binary) - np.sum(y_binary)}\")\n",
        "\n",
        "    # Step 2: Create tasks for meta-learning\n",
        "    print(\"\\n2. Creating few-shot learning tasks...\")\n",
        "    if y_multiclass is not None:\n",
        "        all_tasks = data_processor.create_tasks(X, y_multiclass, num_tasks=200, k_shot=5, query_size=15)\n",
        "\n",
        "        # Split into train, validation and test tasks\n",
        "        num_train = int(len(all_tasks) * 0.7)\n",
        "        num_val = int(len(all_tasks) * 0.15)\n",
        "\n",
        "        train_tasks = all_tasks[:num_train]\n",
        "        val_tasks = all_tasks[num_train:num_train+num_val]\n",
        "        test_tasks = all_tasks[num_train+num_val:]\n",
        "\n",
        "        print(f\"Number of training tasks: {len(train_tasks)}\")\n",
        "        print(f\"Number of validation tasks: {len(val_tasks)}\")\n",
        "        print(f\"Number of test tasks: {len(test_tasks)}\")\n",
        "    else:\n",
        "        print(\"Multiclass labels not available. Cannot create few-shot tasks.\")\n",
        "        return\n",
        "\n",
        "    # Step 3: Initialize MAML model\n",
        "    print(\"\\n3. Initializing MAML Transformer model...\")\n",
        "    input_shape = X.shape[1:]  # Feature dimensions\n",
        "    n_way = min(5, len(np.unique(y_multiclass)))  # Number of classes per task\n",
        "\n",
        "    maml_model = MAMLTransformer(\n",
        "        input_shape=input_shape,\n",
        "        n_way=n_way,\n",
        "        k_shot=5,\n",
        "        inner_lr=0.01,\n",
        "        meta_lr=0.001,\n",
        "        meta_batch_size=16,\n",
        "        num_inner_updates=5,\n",
        "        embed_dim=128,\n",
        "        num_heads=4,\n",
        "        ff_dim=256,\n",
        "        num_transformer_blocks=3,\n",
        "        mlp_units=[64, 32],\n",
        "        dropout=0.1\n",
        "    )\n",
        "\n",
        "    print(f\"Model initialized with {n_way}-way classification\")\n",
        "    print(f\"Input shape: {input_shape}\")\n",
        "\n",
        "    # Step 4: Meta-training\n",
        "    print(\"\\n4. Starting meta-training...\")\n",
        "    trainer = MAMLTrainer(\n",
        "        maml_model=maml_model,\n",
        "        train_tasks=train_tasks,\n",
        "        val_tasks=val_tasks,\n",
        "        test_tasks=test_tasks,\n",
        "        meta_epochs=1000,  # Reduced for demo\n",
        "        meta_batch_size=16,\n",
        "        eval_interval=50,\n",
        "        early_stopping_patience=5,\n",
        "        log_dir='logs/maml_transformer'\n",
        "    )\n",
        "\n",
        "    history = trainer.train()\n",
        "\n",
        "    # Step 5: Visualize training process\n",
        "    print(\"\\n5. Visualizing training history...\")\n",
        "    trainer.visualize_training()\n",
        "\n",
        "    # Step 6: Adaptation analysis\n",
        "    print(\"\\n6. Analyzing adaptation to novel attacks...\")\n",
        "    # Select a random test task for analysis\n",
        "    random_task_idx = np.random.randint(len(test_tasks))\n",
        "    random_task = test_tasks[random_task_idx]\n",
        "\n",
        "    print(f\"Analyzing adaptation curve for task {random_task_idx}\")\n",
        "    adaptation_curve = trainer.plot_adaptation_curve(random_task)\n",
        "\n",
        "    # Step 7: Network security analysis\n",
        "    print(\"\\n7. Performing network security analysis...\")\n",
        "    security_analyzer = NetworkSecurityAnalyzer(\n",
        "        maml_model=maml_model,\n",
        "        data_processor=data_processor,\n",
        "        log_dir='logs/maml_transformer'\n",
        "    )\n",
        "\n",
        "    # Find indices for each attack type\n",
        "    if y_multiclass is not None:\n",
        "        attack_types = np.unique(y_multiclass)\n",
        "        normal_indices = np.where(y_binary == 0)[0]\n",
        "\n",
        "        # Skip normal class (usually labeled as 0)\n",
        "        for attack_idx in attack_types:\n",
        "            if attack_idx == 0:  # Skip normal class\n",
        "                continue\n",
        "\n",
        "            attack_name = f\"Attack Type {attack_idx}\"\n",
        "            print(f\"\\nAnalyzing novel attack detection: {attack_name}\")\n",
        "\n",
        "            # Get indices for this attack type\n",
        "            attack_indices = np.where(y_multiclass == attack_idx)[0]\n",
        "\n",
        "            if len(attack_indices) < 10:\n",
        "                print(f\"Not enough samples for attack type {attack_idx}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Analyze novel attack detection\n",
        "            results = security_analyzer.analyze_novel_attack(\n",
        "                X, y_binary, attack_indices, normal_indices, n_shot=5\n",
        "            )\n",
        "\n",
        "            print(f\"Best adaptation steps: {results['best_steps']}\")\n",
        "            print(f\"Best F1 score: {results['best_f1']:.4f}\")\n",
        "\n",
        "    # Step 8: Cross-attack analysis\n",
        "    print(\"\\n8. Analyzing performance across attack types...\")\n",
        "    # Create tasks for different attack types\n",
        "    attack_tasks = []\n",
        "    attack_names = []\n",
        "\n",
        "    if y_multiclass is not None:\n",
        "        for attack_idx in attack_types:\n",
        "            if attack_idx == 0:  # Skip normal class\n",
        "                continue\n",
        "\n",
        "            # Create a binary classification task (normal vs this attack)\n",
        "            attack_indices = np.where(y_multiclass == attack_idx)[0]\n",
        "\n",
        "            if len(attack_indices) < 10:\n",
        "                continue\n",
        "\n",
        "            # Select 5 examples for support and 15 for query\n",
        "            support_attack = attack_indices[:5]\n",
        "            query_attack = attack_indices[5:20]\n",
        "\n",
        "            # Select normal examples\n",
        "            support_normal = normal_indices[:5]\n",
        "            query_normal = normal_indices[5:20]\n",
        "\n",
        "            # Create support and query sets\n",
        "            support_indices = np.concatenate([support_normal, support_attack])\n",
        "            query_indices = np.concatenate([query_normal, query_attack])\n",
        "\n",
        "            # Create binary labels\n",
        "            support_y = np.concatenate([np.zeros(5), np.ones(5)])\n",
        "            query_y = np.concatenate([np.zeros(15), np.ones(15)])\n",
        "\n",
        "            # Shuffle support set\n",
        "            support_shuffle = np.arange(len(support_y))\n",
        "            np.random.shuffle(support_shuffle)\n",
        "            support_X = X[support_indices][support_shuffle]\n",
        "            support_y = support_y[support_shuffle]\n",
        "\n",
        "            # Create task\n",
        "            task = {\n",
        "                'support_X': support_X,\n",
        "                'support_y': support_y,\n",
        "                'query_X': X[query_indices],\n",
        "                'query_y': query_y,\n",
        "                'n_way': 2,\n",
        "                'k_shot': 5\n",
        "            }\n",
        "\n",
        "            attack_tasks.append(task)\n",
        "            attack_names.append(f\"Attack {attack_idx}\")\n",
        "\n",
        "    if attack_tasks:\n",
        "        performance = security_analyzer.analyze_attack_types(attack_tasks, attack_names)\n",
        "\n",
        "        print(\"\\nPerformance across attack types:\")\n",
        "        for name, acc, f1 in zip(performance['attack_names'], performance['accuracies'], performance['f1_scores']):\n",
        "            print(f\"{name}: Accuracy={acc:.4f}, F1={f1:.4f}\")\n",
        "\n",
        "    # Step 9: Attention visualization\n",
        "    print(\"\\n9. Visualizing attention weights for explainability...\")\n",
        "    security_analyzer.visualize_attention_weights(X, y_multiclass if y_multiclass is not None else y_binary)\n",
        "\n",
        "    print(\"\\nDemo completed successfully!\")\n",
        "    return maml_model, trainer, security_analyzer\n",
        "\n",
        "\n",
        "# Main function to run the entire pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    # Set smaller figures for Jupyter notebooks if needed\n",
        "    plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "    # Run the complete demo\n",
        "    maml_model, trainer, analyzer = run_demo()"
      ],
      "metadata": {
        "id": "YLDzQLC3EYlt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}