{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyMgcu-9Y2Ii"
      },
      "outputs": [],
      "source": [
        "#############################################################\n",
        "# MAML Implementation\n",
        "#############################################################\n",
        "\n",
        "class MAMLTransformer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_shape,\n",
        "        n_way=5,\n",
        "        k_shot=5,\n",
        "        inner_lr=0.01,\n",
        "        meta_lr=0.001,\n",
        "        meta_batch_size=32,\n",
        "        num_inner_updates=5,\n",
        "        embed_dim=256,\n",
        "        num_heads=8,\n",
        "        ff_dim=512,\n",
        "        num_transformer_blocks=4,\n",
        "        mlp_units=[128, 64],\n",
        "        dropout=0.1,\n",
        "        name=\"maml_transformer\"\n",
        "    ):\n",
        "        self.input_shape = input_shape\n",
        "        self.n_way = n_way\n",
        "        self.k_shot = k_shot\n",
        "        self.inner_lr = inner_lr\n",
        "        self.meta_lr = meta_lr\n",
        "        self.meta_batch_size = meta_batch_size\n",
        "        self.num_inner_updates = num_inner_updates\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.num_transformer_blocks = num_transformer_blocks\n",
        "        self.mlp_units = mlp_units\n",
        "        self.dropout = dropout\n",
        "        self.name = name\n",
        "\n",
        "        # Build the model\n",
        "        self.model = self._build_model()\n",
        "\n",
        "        # Initialize optimizer\n",
        "        self.meta_optimizer = keras.optimizers.Adam(learning_rate=self.meta_lr)\n",
        "\n",
        "        # Store the original weights\n",
        "        self.meta_weights = self.model.get_weights()\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"Build the transformer-based model for few-shot learning\"\"\"\n",
        "        inputs = layers.Input(shape=self.input_shape)\n",
        "\n",
        "        # Embedding layer to transform input features to embedding space\n",
        "        x = layers.Dense(self.embed_dim)(inputs)\n",
        "\n",
        "        # Reshape for transformer if input is not sequential\n",
        "        # For sequential data, we would reshape to (batch_size, seq_length, embed_dim)\n",
        "        # For tabular data, we treat each feature as a \"token\"\n",
        "        x = tf.expand_dims(x, axis=1)  # Add sequence dimension of length 1\n",
        "\n",
        "        # Apply positional encoding\n",
        "        x = PositionalEncoding(position=50, d_model=self.embed_dim)(x)\n",
        "\n",
        "        # Transformer blocks\n",
        "        for _ in range(self.num_transformer_blocks):\n",
        "            x = TransformerBlock(\n",
        "                embed_dim=self.embed_dim,\n",
        "                num_heads=self.num_heads,\n",
        "                ff_dim=self.ff_dim,\n",
        "                rate=self.dropout\n",
        "            )(x)\n",
        "\n",
        "        # Global average pooling\n",
        "        x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "        # MLP for classification\n",
        "        for dim in self.mlp_units:\n",
        "            x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "            x = layers.Dropout(self.dropout)(x)\n",
        "\n",
        "        # Output layer\n",
        "        outputs = layers.Dense(self.n_way, activation=\"softmax\")(x)\n",
        "\n",
        "        return keras.Model(inputs, outputs, name=self.name)\n",
        "\n",
        "    def train_on_batch(self, batch_of_tasks):\n",
        "        \"\"\"\n",
        "        Train on a batch of tasks using MAML\n",
        "\n",
        "        Args:\n",
        "            batch_of_tasks: list of tasks, each containing support and query sets\n",
        "        \"\"\"\n",
        "        meta_batch_size = min(len(batch_of_tasks), self.meta_batch_size)\n",
        "\n",
        "        with tf.GradientTape() as meta_tape:\n",
        "            total_meta_loss = tf.constant(0.0, dtype=tf.float32)\n",
        "\n",
        "            for i in range(meta_batch_size):\n",
        "                task = batch_of_tasks[i]\n",
        "                support_X = task['support_X']\n",
        "                support_y = task['support_y']\n",
        "                query_X = task['query_X']\n",
        "                query_y = task['query_y']\n",
        "\n",
        "                # Convert to one-hot\n",
        "                support_y_one_hot = tf.one_hot(support_y, depth=self.n_way)\n",
        "                query_y_one_hot = tf.one_hot(query_y, depth=self.n_way)\n",
        "\n",
        "                # Store the original weights\n",
        "                original_weights = self.model.get_weights()\n",
        "\n",
        "                # Inner loop - adapt to the task\n",
        "                for _ in range(self.num_inner_updates):\n",
        "                    with tf.GradientTape() as inner_tape:\n",
        "                        support_logits = self.model(support_X, training=True)\n",
        "                        support_loss = keras.losses.categorical_crossentropy(\n",
        "                            support_y_one_hot, support_logits\n",
        "                        )\n",
        "                        support_loss = tf.reduce_mean(support_loss)\n",
        "\n",
        "                    # Compute gradients and update model weights\n",
        "                    gradients = inner_tape.gradient(support_loss, self.model.trainable_variables)\n",
        "\n",
        "                    # Manual weight update for inner loop\n",
        "                    updated_weights = []\n",
        "                    for j, (weight, grad) in enumerate(zip(self.model.get_weights(), gradients)):\n",
        "                        if grad is not None:\n",
        "                            updated_weights.append(weight - self.inner_lr * grad)\n",
        "                        else:\n",
        "                            updated_weights.append(weight)\n",
        "\n",
        "                    # Set the updated weights\n",
        "                    self.model.set_weights(updated_weights)\n",
        "\n",
        "                # Evaluate on query set with the adapted model\n",
        "                query_logits = self.model(query_X, training=True)\n",
        "                query_loss = keras.losses.categorical_crossentropy(\n",
        "                    query_y_one_hot, query_logits\n",
        "                )\n",
        "                query_loss = tf.reduce_mean(query_loss)\n",
        "\n",
        "                # Add to meta loss\n",
        "                total_meta_loss += query_loss\n",
        "\n",
        "                # Restore original weights\n",
        "                self.model.set_weights(original_weights)\n",
        "\n",
        "            # Average meta loss\n",
        "            total_meta_loss /= meta_batch_size\n",
        "\n",
        "        # Compute gradients of meta loss with respect to meta weights\n",
        "        meta_gradients = meta_tape.gradient(total_meta_loss, self.model.trainable_variables)\n",
        "\n",
        "        # Apply meta gradients\n",
        "        self.meta_optimizer.apply_gradients(zip(meta_gradients, self.model.trainable_variables))\n",
        "\n",
        "        # Update stored meta weights\n",
        "        self.meta_weights = self.model.get_weights()\n",
        "\n",
        "        return total_meta_loss.numpy()\n",
        "\n",
        "    def evaluate(self, tasks, num_inner_updates=None):\n",
        "        \"\"\"\n",
        "        Evaluate the meta-model on a list of tasks\n",
        "\n",
        "        Args:\n",
        "            tasks: list of tasks to evaluate on\n",
        "            num_inner_updates: number of inner updates to perform (if None, use self.num_inner_updates)\n",
        "\n",
        "        Returns:\n",
        "            mean accuracy across tasks\n",
        "        \"\"\"\n",
        "        if num_inner_updates is None:\n",
        "            num_inner_updates = self.num_inner_updates\n",
        "\n",
        "        accuracies = []\n",
        "        losses = []\n",
        "\n",
        "        # Restore meta weights\n",
        "        self.model.set_weights(self.meta_weights)\n",
        "\n",
        "        for task in tasks:\n",
        "            support_X = task['support_X']\n",
        "            support_y = task['support_y']\n",
        "            query_X = task['query_X']\n",
        "            query_y = task['query_y']\n",
        "\n",
        "            # Convert to one-hot\n",
        "            support_y_one_hot = tf.one_hot(support_y, depth=self.n_way)\n",
        "\n",
        "            # Store the original weights\n",
        "            original_weights = self.model.get_weights()\n",
        "\n",
        "            # Inner loop adaptation\n",
        "            for _ in range(num_inner_updates):\n",
        "                with tf.GradientTape() as tape:\n",
        "                    support_logits = self.model(support_X, training=True)\n",
        "                    support_loss = keras.losses.categorical_crossentropy(\n",
        "                        support_y_one_hot, support_logits\n",
        "                    )\n",
        "                    support_loss = tf.reduce_mean(support_loss)\n",
        "\n",
        "                # Compute gradients and update model weights\n",
        "                gradients = tape.gradient(support_loss, self.model.trainable_variables)\n",
        "\n",
        "                # Manual weight update\n",
        "                updated_weights = []\n",
        "                for j, (weight, grad) in enumerate(zip(self.model.get_weights(), gradients)):\n",
        "                    if grad is not None:\n",
        "                        updated_weights.append(weight - self.inner_lr * grad)\n",
        "                    else:\n",
        "                        updated_weights.append(weight)\n",
        "\n",
        "                # Set the updated weights\n",
        "                self.model.set_weights(updated_weights)\n",
        "\n",
        "            # Evaluate on query set\n",
        "            query_logits = self.model(query_X, training=False)\n",
        "            pred_y = tf.argmax(query_logits, axis=1).numpy()\n",
        "            accuracy = np.mean(pred_y == query_y)\n",
        "\n",
        "            # Calculate loss\n",
        "            query_y_one_hot = tf.one_hot(query_y, depth=self.n_way)\n",
        "            loss = keras.losses.categorical_crossentropy(query_y_one_hot, query_logits)\n",
        "            loss = tf.reduce_mean(loss).numpy()\n",
        "\n",
        "            accuracies.append(accuracy)\n",
        "            losses.append(loss)\n",
        "\n",
        "            # Restore original weights\n",
        "            self.model.set_weights(original_weights)\n",
        "\n",
        "        # Return mean accuracy and loss\n",
        "        return np.mean(accuracies), np.mean(losses)\n",
        "\n",
        "    def adapt_to_task(self, support_X, support_y, num_inner_updates=None):\n",
        "        \"\"\"\n",
        "        Adapt the model to a new task using the support set\n",
        "\n",
        "        Args:\n",
        "            support_X: support set features\n",
        "            support_y: support set labels\n",
        "            num_inner_updates: number of inner updates to perform (if None, use self.num_inner_updates)\n",
        "\n",
        "        Returns:\n",
        "            Adapted model\n",
        "        \"\"\"\n",
        "        if num_inner_updates is None:\n",
        "            num_inner_updates = self.num_inner_updates\n",
        "\n",
        "        # Convert class indices to one-hot\n",
        "        support_y_one_hot = tf.one_hot(support_y, depth=self.n_way)\n",
        "\n",
        "        # Reset to the meta weights\n",
        "        self.model.set_weights(self.meta_weights)\n",
        "\n",
        "        # Inner loop adaptation\n",
        "        for _ in range(num_inner_updates):\n",
        "            with tf.GradientTape() as tape:\n",
        "                support_logits = self.model(support_X, training=True)\n",
        "                support_loss = keras.losses.categorical_crossentropy(\n",
        "                    support_y_one_hot, support_logits\n",
        "                )\n",
        "                support_loss = tf.reduce_mean(support_loss)\n",
        "\n",
        "            # Compute gradients\n",
        "            gradients = tape.gradient(support_loss, self.model.trainable_variables)\n",
        "\n",
        "            # Manual weight update\n",
        "            updated_weights = []\n",
        "            for j, (weight, grad) in enumerate(zip(self.model.get_weights(), gradients)):\n",
        "                if grad is not None:\n",
        "                    updated_weights.append(weight - self.inner_lr * grad)\n",
        "                else:\n",
        "                    updated_weights.append(weight)\n",
        "\n",
        "            # Set the updated weights\n",
        "            self.model.set_weights(updated_weights)\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def save_meta_model(self, filepath):\n",
        "        \"\"\"Save the meta-model weights\"\"\"\n",
        "        # Set to meta weights before saving\n",
        "        self.model.set_weights(self.meta_weights)\n",
        "        self.model.save_weights(filepath)\n",
        "\n",
        "    def load_meta_model(self, filepath):\n",
        "        \"\"\"Load the meta-model weights\"\"\"\n",
        "        self.model.load_weights(filepath)\n",
        "        self.meta_weights = self.model.get_weights()\n"
      ]
    }
  ]
}